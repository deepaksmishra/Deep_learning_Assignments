{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8hHZHbmaGkQ"
      },
      "source": [
        "1. What are the main tasks that autoencoders are used for?\n",
        "Ans : \n",
        "Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible\n",
        "\n",
        "2. Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed?\n",
        "Ans: \n",
        "Autoencoders are a specific type of feedforward neural networks where the input is the same as the output. They compress the input into a lower-dimensional code and then reconstruct the output from this representation. The code is a compact “summary” or “compression” of the input, also called the latent-space representation.\n",
        "An autoencoder consists of 3 components: encoder, code and decoder. The encoder compresses the input and produces the code, the decoder then reconstructs the input only using this code.\n",
        "There are 4 hyperparameters that we need to set before training an autoencoder:\n",
        "Code size: number of nodes in the middle layer. Smaller size results in more compression.\n",
        "Number of layers: the autoencoder can be as deep as we like. In the figure above we have 2 layers in both the encoder and decoder, without considering the input and output.\n",
        "Number of nodes per layer: the autoencoder architecture we’re working on is called a stacked autoencoder since the layers are stacked one after another. Usually stacked autoencoders look like a “sandwitch”. The number of nodes per layer decreases with each subsequent layer of the encoder, and increases back in the decoder. Also the decoder is symmetric to the encoder in terms of layer structure. As noted above this is not necessary and we have total control over these parameters.\n",
        "Loss function: we either use mean squared error (mse) or binary crossentropy. If the input values are in the range [0, 1] then we typically use crossentropy, otherwise we use the mean squared error. \n",
        "\n",
        "\n",
        "3.If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?\n",
        "Ans\n",
        "If you consider conventional autoencoder function, yes, it is a good autoencoder. In practice, efficiency of autoencoder depends on how well it reconstructs and also on how robust it is to noise in different scenes.\n",
        "\n",
        "Common practice is to add noise sampled from input distribution to the input space to make sure autoencoder, vanilla or VAE, learns to reconstruct the input more robustly regardless of scenic distortions.\n",
        "\n",
        "However, maybe your goal never was reconstruction and thus it doesn’t matter how good reconstruction is. Maybe you wanted to learn features and leverage it for other use. In that case, you wouldn’t care, mostly, about how well reconstruction happens. It is known that noise in input space doesn’t necessary help in better converage of feature space and thus feature learning is hampered. So, community came up with idea of introducing noise in the feature space instead of input space. It will obviously hurt the reconstruction but definitely learned features would be better and overall your feature vector would be more definitive of the latent space as a whole.\n",
        "\n",
        "4.What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?\n",
        "Ans: \n",
        "Autoencoders are neural network trained in an unsupervised way to attempt to copy inputs to outputs.\n",
        "Throughout the training phase, the goal is for our network to be able to learn how to reconstruct our input data. The following figure illustrates this idea by showing the Autoencoder model architecture.\n",
        "However, most of the time, it is not the output of the decoder that interests us but rather the latent space representation. We hope that training the Autoencoder end-to-end will then allow our encoder to find useful features in our data.\n",
        "The decoder, , is used to train the autoencoder end-to-end, but in practical applications, we often care more about the encoder and the codings.\n",
        "To highlight important properties, one can, for example, constrain the latent space to be smaller than the dimension of the inputs. In this case, our model is an Undercomplete Autoencoders. In the majority of cases we work with this type of autoencoders since one of the main applications of this architecture is dimensionality reduction.\n",
        "\n",
        "5.How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
        "Ans: An autoencoder with tied weights has decoder weights that are the transpose of the encoder weights; this is a form of parameter sharing, which reduces the number of parameters of the model.\n",
        "Advantages of tying weights include increased training speed and reduced risk of overfitting, while yielding comparable performance than without weight tying in many cases (Li et al. (2019)). It is therefore a common practice to tie weights when building a symmetrical autoencoder.\n",
        "\n",
        "6.What is a generative model? Can you name a type of generative autoencoder?\n",
        "Ans: \n",
        "\n",
        "Generative models have gained much popularity in recent years. These models help in handling missing information as well as treating with the variable-length sequences. Technically speaking, generative models deal with the models of distributions, defined over data points in some potentially high-dimensional space.\n",
        "\n",
        "we discuss seven types of generative models, which are listed below in alphabetical order-\n",
        "Autoregressive Models\n",
        "Bayesian Network\n",
        "Generative Adversarial Networks\n",
        "\n",
        "\n",
        "7.What is a GAN? Can you name a few tasks where GANs can shine?\n",
        "Ans: Right, we have a sense of what GANs can do. But how do they work? What goes on underneath all the wonderful applications this powerful algorithm produces? Let’s understand this using a popular example.\n",
        "\n",
        "There’s a forger (who creates fake artistry) and an investigator tasked with detecting these fake artworks.\n",
        "Applications of GANs\n",
        "Now that we have an intuition of how GANs work, let’s put on our exploration hats! It’s time to dive into the interesting applications of GANs that are commonly used in the industry right now.\n",
        "\n",
        "8.What are the main difficulties when training GANs?\n",
        "Ans: GAN samples noise z using normal or uniform distribution and utilizes a deep network generator G to create an image x (x=G(z)).\n",
        "Non-convergence: the model parameters oscillate, destabilize and never converge,\n",
        "Mode collapse: the generator collapses which produces limited varieties of samples,\n",
        "Diminished gradient: the discriminator gets too successful that the generator gradient vanishes and learns nothing,\n",
        "Unbalance between the generator and discriminator causing overfitting, &\n",
        "Highly sensitive to the hyperparameter selections."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}