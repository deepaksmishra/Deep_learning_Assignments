{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9AAtSkrcR0_"
      },
      "source": [
        "1 What does a SavedModel contain? How do you inspect its content?\n",
        "Ans: \n",
        "A SavedModel contains a complete TensorFlow program, including trained parameters (i.e, tf.Variables) and computation. It does not require the original model building code to run,\n",
        " which makes it useful for sharing or deploying with TFLite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub.\n",
        "\n",
        "2 When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
        "Ans: \n",
        "TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.\n",
        "Why TensorFlow Serving?\n",
        "If you are used to building your models using TensorFlow or Keras, then the easiest way of deploying your models is by using TensorFlow Serving.\n",
        "Most of the TensorFlow documentation is written for TensorFlow-1.0 and it sadly doesn’t work as is for TensorFlow-2.0. Hence the need for this blog.\n",
        "\n",
        "\n",
        "3 How do you deploy a model across multiple TF Serving instances?\n",
        "Ans: \n",
        "Step 1: Install the Docker App.\n",
        "Step 2: Pull the TensorFlow Serving Image. docker pull tensorflow/serving. \n",
        "Step 3: Create and Train the Model. \n",
        "Step 4: Save the Model. \n",
        "Step 5: Serving the model using Tensorflow Serving. \n",
        "Step 6: Make a REST request the model to predict.\n",
        "\n",
        "\n",
        "\n",
        "4 When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
        "Ans: \n",
        "GRPC is a modern open-source, high-performance, low latency and high speed throughput RPC framework that uses HTTP/2 as transport protocol and uses protocol buffers as the Interface Definition Language(IDL) and also as its underlying message interchange format\n",
        "\n",
        "5 What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?\n",
        "Ans: \n",
        "A large model size is a common byproduct when attempting to push the limits of model accuracy in predicting unseen data in deep learning applications. For example, with more nodes, we can detect subtler features in the dataset. However, for project requirements such as using AI in embedded systems that depend on fast predictions, we are limited by the available computational resources. Furthermore, prevailing edge devices do not have networking capabilities, as such, we are not able to utilize cloud computing. This results in the inability to use massive models which would take too long to get meaningful predictions.\n",
        "\n",
        "\n",
        "6 What is quantization-aware training, and why would you need it?\n",
        "Ans: \n",
        "As we move to a lower precision from float, we generally notice a significant accuracy drop as this is a lossy process. This loss can be minimized with the help of quant-aware training. So basically, quant-aware training simulates low precision behavior in the forward pass, while the backward pass remains the same. This induces some quantization error which is accumulated in the total loss of the model and hence the optimizer tries to reduce it by adjusting the parameters accordingly. This makes our parameters more robust to quantization making our process almost lossless.\n",
        "\n",
        "7 What are model parallelism and data parallelism? Why is the latter generally recommended?\n",
        "Ans: \n",
        "Several restrictions exist in both data parallelism and model parallelism. For data parallelism, we have to reduce the learning rate to keep a smooth training process if there are too many computational nodes. For model parallelism, the performance of the network will be dramatically decreased for the sake of communication expense if we have too many nodes.\n",
        "\n",
        "Model parallelism could get a good performance with a large number of neuron activities, and data parallel is efficient with large number of weights. In CNNs, the convolution layer contain about 90% of the computation and 5% of the parameters, while the full connected layer contain 95% of the parameters and 5%-10% the computation. Therefore, we can parallelize the CNNs in data-model mode by using data parallelism for convolutional layer and model parallelism for a fully connected layer\n",
        "\n",
        "\n",
        "8 When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
        "Ans: \n",
        "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
        "\n",
        "tf.distribute.Strategy has been designed with these key goals in mind:\n",
        "\n",
        "Easy to use and support multiple user segments, including researchers, machine learning engineers, etc.\n",
        "Provide good performance out of the box.\n",
        "Easy switching between strategies.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}