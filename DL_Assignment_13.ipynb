{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mig2wr9p2i4P"
      },
      "source": [
        "1 Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
        "Ans: \n",
        "\n",
        "The logistic regression classifier is able to converge on non-linear data and outputs class probabilities.\n",
        "\n",
        "2 Why was the logistic activation function a key ingredient in training the first MLPs?\n",
        "Ans: \n",
        "Because the derivative of the logistic function is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all.\n",
        "\n",
        "\n",
        "3 Name three popular activation functions. Can you draw them?\n",
        "Ans: Losgistic. Relu. Tanh. Suppose you have an MLP composed of one input layer with 10 passthrough\n",
        "\n",
        " Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
        " What is the shape of the input matrix X?\n",
        "5 How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function?\n",
        "Ans: You need 10 neurons in the output layer, and you must use the softmax activation function, which can handle multiple classes, outputting one probability per class.\n",
        "\n",
        "\n",
        "6 What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
        "Ans: However I am not sure how this differs from the reverse-mode autodiff implementation by TensorFlow.\n",
        "\n",
        "As far as I know reverse-mode autodiff first goes through the graph in the forward direction and then in the second pass computes all partial derivatives for the outputs with respect to the inputs. This is very similar to the propagation algorithm.\n",
        "\n",
        "\n",
        "7 Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "Ans: Birds inspired us to fly, burdock plants inspired velcro, and nature has inspired many other inventions. It seems only logical, then, to look at the brain’s architecture for inspiration on how to build an intelligent machine. This is the key idea that inspired artificial neural networks (ANNs). However, although planes were inspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying “units” rather than “neurons”), lest we restrict our creativity to biologically plausible systems.1\n",
        "\n",
        "ANNs are at the very core of Deep Learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex Machine Learning tasks, such as classifying billions of images (e.g., Google Images), powering speech recognition services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds of millions of users every day (e.g., YouTube), or learning to beat the world champion at the game of Go by examining millions of past games and then playing against itself (DeepMind’s AlphaGo).\n",
        "\n",
        "8 Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on).\n",
        "Ans: For the output layer, the softmax activation function is generally a good choice for classification tasks(when the classes are mutually exclusive). For regression tasks, you can simply use no activation functionat all.This concludes this introduction to artificial neural networks. In the following chapters, we will discusstechniques to train very deep nets, and distribute training across multiple servers and GPUs. Then we willexplore a few other popular neural network architectures: convolutional neural networks, recurrent neuralnetworks, and autoencoders.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}