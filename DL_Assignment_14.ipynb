{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_14.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNL1mJBc_2BI"
      },
      "source": [
        "1 is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
        "Ans: \n",
        "Building even a simple neural network can be a confusing task and upon that tuning it to get a better result is extremely tedious. But, the first step that comes in consideration while building a neural network is the initialization of parameters, if done correctly then optimization will be achieved in the least time otherwise converging to a minima using gradient descent will be impossible.\n",
        "This article has been written under the assumption that the reader is already familiar with the concept of neural network, weight, bias, activation functions, forward and backward propagation etc.\n",
        "\n",
        "2 Is it okay to initialize the bias terms to 0?\n",
        "Ans: \n",
        "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights\n",
        "\n",
        "\n",
        "3 Name three advantages of the ELU activation function over ReLU.\n",
        "Ans: \n",
        "Advantages:\n",
        "\n",
        "It isn’t limited by the ‘zero dying’ problem because it doesn’t have 0 slope(the -ve part).\n",
        "Leaky ReLU is a bit more balanced(also the negative values are near 0) so the overall training time of the model is low.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Unlike PReLU, the coefficient of x is predefined and the Neural Network doesn’t decide the coefficient itself.\n",
        "ELU\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Doesn’t have the dying ReLU problem.\n",
        "The function tends to converge cost to zero faster and produces more accurate results(recently read it on a blog).\n",
        "More of a merger between good features of ReLU & Leaky ReLU.\n",
        "Disadvantages:\n",
        "\n",
        "Well, saturates for large negative values.\n",
        "Hope this helps!\n",
        "\n",
        "I’ll be following this question. If someone else is able to provide a more detailed answer, it shall help me learn a few things as well.\n",
        "\n",
        "\n",
        "4 In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "Ans: \n",
        "Let's start with the basics of Neurons and Neural Network and What is an Activation Function and Why we would need it : First proposed in 1944 by Warren\n",
        "\n",
        "\n",
        "5 What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
        "Ans: Exponentially weighed averages deal with sequences of numbers. Suppose, we have some sequence S which is noisy. For this example I plotted cosine function and added some Gaussian noise. It looks like this:\n",
        "\n",
        "\n",
        "6 Name three ways you can produce a sparse model.\n",
        "Ans: Sparse Convolution plays an essential role in LiDAR signal processing. This article describes how the sparse convolution works, which used a quite different concept and GPU calculation schema compared with traditional convolution.\n",
        "\n",
        "\n",
        "7 Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
        "Ans: Dropout is one of the most popular regularization techniques for deep neural networks. It was proposed in a paper23 by Geoffrey Hinton in 2012 and further detailed in a 2014 paper24 by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}