{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignmnet_12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Sa_eann9Ie"
      },
      "source": [
        "1 How does unsqueeze help us to solve certain broadcasting problems?\n",
        "Ans: \n",
        "It indicates the position on where to add the dimension. torch.unsqueeze adds an additional dimension to the tensor.\n",
        "\n",
        "So let's say you have a tensor of shape (3), if you add a dimension at the 0 position, it will be of shape (1,3), which means 1 row and 3 columns:\n",
        "\n",
        "If you have a 2D tensor of shape (2,2) add add an extra dimension at the 0 position, this will result of the tensor having a shape of (1,2,2), which means one channel, 2 rows and 2 columns. If you add at the 1 position, it will be of shape (2,1,2), so it will have 2 channels, 1 row and 2 columns.\n",
        "If you add at the 1 position, it will be (3,1), which means 3 rows and 1 column.\n",
        "If you add it at the 2 position, the tensor will be of shape (2,2,1), which means 2 channels, 2 rows and one column.\n",
        "\n",
        "\n",
        "\n",
        "2 How can we use indexing to do the same operation as unsqueeze?\n",
        "Ans: \n",
        "Indexing a tensor is like indexing a normal Python list. Indexing multiple dimensions can be done by recursively indexing each dimension. Indexing chooses the index from the first available dimension. Each dimension can be separated while indexing by using a comma. You can use this method when doing slicing. Start and end indices can be separated using a full colon. The transpose of a matrix can be accessed using the attribute t; every PyTorch tensor object has the attribute t.\n",
        "\n",
        "Concatenation is another important operation that you need in your toolbox. PyTorch made the function cat for the same purpose. Two tensors of the same size on all the dimensions except one, if required, can be concatenated using cat. For example, a tensor of size 3 x 2 x 4 can be concatenated with another tensor of size 3 x 5 x 4 on the first dimension to get a tensor of size 3 x 7 x 4. The stack operation looks very similar to concatenation but it is an entirely different operation. If you want to add a new dimension to your tensor, stack is the way to go. Similar to cat, you can pass the axis where you want to add the new dimension. However, make sure all the dimensions of the two tensors are the same other than the attaching dimension.\n",
        "\n",
        "split and chunk are similar operations for splitting your tensor. split accepts the size you want each output tensor to be. For example, if you are splitting a tensor of size 3 x 2 with size 1 in the 0th dimension, you'll get three tensors each of size 1 x 2. However, if you give 2 as the size on the zeroth dimension, you'll get a tensor of size 2 x 2 and another of size 1 x 2.\n",
        "\n",
        "The squeeze function sometimes saves you hours of time. There are situations where you'll have tensors with one or more dimension size as 1. Sometimes, you don't need those extra dimensions in your tensor. That is where squeeze is going to help you. squeeze removes the dimension with value 1. For example, if you are dealing with sentences and you have a batch of 10 sentences with five words each, when you map that to a tensor object, you'll get a tensor of 10 x 5. Then you realize that you have to convert that to one-hot vectors for your neural network to process.\n",
        "\n",
        "\n",
        "3 How do we show the actual contents of the memory used for a tensor?\n",
        "ans: \n",
        "To get a value from single element tensor x.item() works always:\n",
        "Example : Single element tensor on CPU\n",
        "\n",
        "x = torch.tensor([3])\n",
        "x.item()\n",
        "Output:\n",
        "\n",
        "3\n",
        "Example : Single element tensor on CPU with AD\n",
        "\n",
        "x = torch.tensor([3.], requires_grad=True)\n",
        "x.item()\n",
        "\n",
        "\n",
        "4 When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
        "ans:# Load libraries\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "# Create a matrix\n",
        "matrix = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [3, 0]])\n",
        "\n",
        "# Create compressed sparse row (CSR) matrix\n",
        "matrix_sparse = sparse.csr_matrix(matrix)\n",
        "\n",
        "\n",
        "\n",
        "5 Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
        "Ans: \n",
        "initial memory allocations are done without any problems. However, when I try to perform the subtract operation with broadcasting, the memory grows to more than 100GB. I always thought that broadcasting would avoid making extra memory allocations but now I am not sure if this is always the case.\n",
        "\n",
        "\n",
        "6 Implement matmul using Einstein summation.\n",
        "Ans: \n",
        "Einsum is implemented in numpy via np.einsum, in PyTorch via torch.einsum, and in TensorFlow via tf.einsum.6 All three einsum functions share the same signature einsum(equation,operands) where equation is a string representing the Einstein summation and operands is a sequence of tensors.7 The examples above can all be written using an equation string. For instance, our first example cj=∑i∑kAikBkj can be written as the equation string \"ik,kj -> j\". Note that the naming of the indices (i, j, k) is arbitrary but it needs to be used consistently.\n",
        "\n",
        "What's great about having einsum not only in numpy but also in PyTorch and TensorFlow is that it can be used in arbitrary computation graphs for neural network architectures and that we can backpropagate through it. A typical call to einsum has the following form\n",
        "result=einsum(\"□□,□□□,□□->□□\",arg1,arg2,arg3)\n",
        "where □ is a placeholder for a character identifying a tensor dimension. From this equation string we can infer that arg1 and arg3 are matrices, arg2 is an order-3 tensor, and that the result of this einsum operation is a matrix. Note that einsum works with a variable number of inputs. In the example above, einsum specifies an operation on three arguments, but it can also be used for operations involving one, two or more than three arguments. Einsum is best learned by studying examples, so let's go through some examples for einsum in PyTorch that correspond to library functions which are used in many deep learning models.\n",
        "\n",
        "\n",
        "7 What does a repeated index letter represent on the lefthand side of einsum?\n",
        "Ans: \n",
        "Imagine that we have two multi-dimensional arrays, A and B. Now let's suppose we want to...\n",
        "\n",
        "multiply A with B in a particular way to create new array of products; and then maybe\n",
        "sum this new array along particular axes; and then maybe\n",
        "transpose the axes of the new array in a particular order.\n",
        "There's a good chance that einsum will help us do this faster and more memory-efficiently than combinations of the NumPy functions like multiply, sum and transpose will allow.\n",
        "\n",
        "How does einsum work?\n",
        "Here's a simple (but not completely trivial) example. Take the following two arrays:\n",
        "\n",
        "A = np.array([0, 1, 2])\n",
        "\n",
        "B = np.array([[ 0,  1,  2,  3],\n",
        "              [ 4,  5,  6,  7],\n",
        "              [ 8,  9, 10, 11]])\n",
        "We will multiply A and B element-wise and then sum along the rows of the new array. In \"normal\" NumPy we'd write:\n",
        "\n",
        ">>> (A[:, np.newaxis] * B).sum(axis=1)\n",
        "array([ 0, 22, 76])\n",
        "So here, the indexing operation on A lines up the first axes of the two arrays so that the multiplication can be broadcast. The rows of the array of products are then summed to return the answer.\n",
        "\n",
        "Now if we wanted to use einsum instead, we could write:\n",
        "\n",
        ">>> np.einsum('i,ij->i', A, B)\n",
        "array([ 0, 22, 76])\n",
        "The signature string 'i,ij->i' is the key here and needs a little bit of explaining. You can think of it in two halves. On the left-hand side (left of the ->) we've labelled the two input arrays. To the right of ->, we've labelled the array we want to end up with.\n",
        "\n",
        "Here is what happens next:\n",
        "\n",
        "A has one axis; we've labelled it i. And B has two axes; we've labelled axis 0 as i and axis 1 as j.\n",
        "\n",
        "By repeating the label i in both input arrays, we are telling einsum that these two axes should be multiplied together. In other words, we're multiplying array A with each column of array B, just like A[:, np.newaxis] * B does.\n",
        "\n",
        "Notice that j does not appear as a label in our desired output; we've just used i (we want to end up with a 1D array). By omitting the label, we're telling einsum to sum along this axis. In other words, we're summing the rows of the products, just like .sum(axis=1) does.\n",
        "\n",
        "That's basically all you need to know to use einsum. It helps to play about a little; if we leave both labels in the output, 'i,ij->ij', we get back a 2D array of products (same as A[:, np.newaxis] * B). If we say no output labels, 'i,ij->, we get back a single number (same as doing (A[:, np.newaxis] * B).sum()).\n",
        "\n",
        "The great thing about einsum however, is that it does not build a temporary array of products first; it just sums the products as it goes. This can lead to big savings in memory use.\n",
        "\n",
        "A slightly bigger example\n",
        "To explain the dot product, here are two new arrays:\n",
        "\n",
        "A = array([[1, 1, 1],\n",
        "           [2, 2, 2],\n",
        "           [5, 5, 5]])\n",
        "\n",
        "B = array([[0, 1, 0],\n",
        "           [1, 1, 0],\n",
        "           [1, 1, 1]])\n",
        "We will compute the dot product using np.einsum('ij,jk->ik', A, B). Here's a picture showing the labelling of the A and B and the output array that we get from the function:\n",
        "\n",
        "\n",
        "8 What are the three rules of Einstein summation notation? Why?\n",
        "Ans: \n",
        "1. Repeated indices are implicitly summed over.\n",
        "\n",
        "2. Each index can appear at most twice in any term.\n",
        "\n",
        "3. Each term must contain identical non-repeated indices.\n",
        "\n",
        "\n",
        "9 What are the forward pass and backward pass of a neural network?\n",
        "Ans: \n",
        "The \"forward pass\" refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.\n",
        "\n",
        "A loss function is calculated from the output values.\n",
        "\n",
        "And then \"backward pass\" refers to process of counting changes in weights (de facto learning), using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n",
        "\n",
        "Backward and forward pass makes together one \"iteration\".\n",
        "\n",
        "\n",
        "10 Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
        "Ans: We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.\n",
        "\n",
        "\n",
        "\n",
        "11 What is the downside of having activations with a standard deviation too far away from 1?\n",
        "Ans:  Artificial neuron is not similar in working as compared to biological neuron since artificial neuron first takes a weighted sum of all inputs along with bias followed by applying an activation function to gives the final result whereas the working of biological neuron involves axon, synapses, etc.\n",
        "\n",
        "\n",
        "12 How can weight initialization help avoid this problem?\n",
        "Ans: Zero initialization :\n",
        "In general practice biases are initialized with 0 and weights are initialized with random numbers, what if weights are initialized with 0?\n",
        "In order to understand let us consider we applied sigmoid activation function for the output layer.\n",
        "Random initialization :\n",
        "Assigning random values to weights is better than just 0 assignment. But there is one thing to keep in my mind is that what happens if weights are initialized high values or very low values and what is a reasonable initialization of weight values.\n",
        "a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time.\n",
        "b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above.\n",
        "This problem is often referred to as the vanishing gradient.\n",
        "To see this let us see the example we took above but now the weights are initialized with very large values instead of 0 :\n",
        "W[l] = np.random.randn(l-1,l)*10\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}