{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dl_Assignment_16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOCQdhx-CnPC"
      },
      "source": [
        "1. Explain the Activation Functions in your own language\n",
        "sigmoid\n",
        "A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly one inflection point. A sigmoid \"function\" and a sigmoid \"curve\" refer to the same object.\n",
        "\n",
        "tanh\n",
        "Tanh is the hyperbolic tangent function, which is the hyperbolic analogue of the Tan circular function used throughout trigonometry. Tanh[α] is defined as the ratio of the corresponding hyperbolic sine and hyperbolic cosine functions via . Tanh may also be defined as , where is the base of the natural logarithm Log.\n",
        "\n",
        "ReLU\n",
        "In the context of artificial neural networks, the rectifier or ReLU activation function is an activation function defined as the positive part of its argument: {\\displaystyle f(x)=x^{+}=\\max} where x is the input to a neuron.\n",
        "\n",
        "\n",
        "ELU\n",
        "ELU. Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number. ... They are both in identity function form for non-negative inputs.\n",
        "\n",
        "LeakyReLU\n",
        "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training.\n",
        "\n",
        "swish\n",
        ": to move, pass, swing, or whirl with the sound of a swish. transitive verb. 1 : to move, cut, or strike with a swish the horse swished its tail. 2 : to make (a basketball shot) so that the ball falls through the rim without touching it swished a 3-point jumper.\n",
        "\n",
        "2. What happens when you increase or decrease the optimizer learning rate?\n",
        "Ans: \n",
        "Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm.\n",
        "\n",
        "The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
        "\n",
        "\n",
        "3. What happens when you increase the number of internal hidden neurons?\n",
        "Ans: \n",
        "An inordinately large number of neurons in the hidden layers can increase the time it takes to train the network. The amount of training time can increase to the point that it is impossible to adequately train the neural network.\n",
        "\n",
        "\n",
        "4. What happens when you increase the size of batch computation?\n",
        "Ans: \n",
        "Finding: large batch size means the model makes very large gradient updates and very small gradient updates. The size of the update depends heavily on which particular samples are drawn from the dataset. On the other hand using small batch size means the model makes updates that are all about the same size.\n",
        "\n",
        "5. Why we adopt regularization to avoid overfitting?\n",
        "Ans: The first step when dealing with overfitting is to decrease the complexity of the model. To decrease the complexity, we can simply remove layers or reduce the number of neurons to make the network smaller. While doing this, it is important to calculate the input and output dimensions of the various layers involved in the neural network. There is no general rule on how much to remove or how large your network should be. But, if your neural network is overfitting, try making it smaller.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. What are loss and cost functions in deep learning?\n",
        "Ans: \n",
        "A loss function is for a single training example. It is also sometimes called an error function. A cost function, on the other hand, is the average loss over the entire training dataset. The optimization strategies aim at minimizing the cost function.\n",
        "\n",
        "\n",
        "7. What do ou mean by underfitting in neural networks?\n",
        "Ans: Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data\n",
        "\n",
        "\n",
        "8. Why we use Dropout in Neural Networks?\n",
        "Ans: — Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. Dropout simulates a sparse activation from a given layer, which interestingly, in turn, encourages the network to actually learn a sparse representation as a side-effect.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}